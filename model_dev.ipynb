{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# \n",
    "df = pd.read_parquet('./audio_dataset_bytes_normalized.parquet')\n",
    "metadata = np.load('./metadata_normalized.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_frames = metadata['max_frames']\n",
    "n_features = metadata['n_features']\n",
    "\n",
    "# transform byte-like back to 2D array\n",
    "X_list = []\n",
    "y_list = []\n",
    "for _, row in df.iterrows():\n",
    "    shape = row['feature_shape']  \n",
    "    feature = np.frombuffer(row['feature_bytes'], dtype=np.float32)\n",
    "    feature = feature.reshape(shape)  # reshape to 2D array\n",
    "    X_list.append(feature)\n",
    "    y_list.append(row['label'])\n",
    "\n",
    "X = np.array(X_list)  \n",
    "y = np.array(y_list)\n",
    "\n",
    "# add channel (CNN need 4D tensor)\n",
    "X = np.expand_dims(X, -1)  \n",
    "\n",
    "# transform to one-hot encoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "y_onehot = tf.keras.utils.to_categorical(y_encoded, num_classes)\n",
    "\n",
    "# split train:test in ratio of 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import models, layers, callbacks, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# residual block (dilated convolution) function\n",
    "def residual_block(x, filters, kernel_size, dilation_rate=1, dropout_rate=0.3, regularizer=regularizers.l2(0.001)):\n",
    "    shortcut = x\n",
    "    # Convolution 1 with dilated convolution\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu',\n",
    "                      dilation_rate=dilation_rate, kernel_regularizer=regularizer)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # Convolution 2 without activation before BatchNormalization\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', activation=None,\n",
    "                      dilation_rate=dilation_rate, kernel_regularizer=regularizer)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# Attention Block Function\n",
    "def attention_block(inputs):\n",
    "    # inputs shape: (batch_size, time_steps, features)\n",
    "    score = layers.Dense(inputs.shape[-1], activation='tanh')(inputs)  # (batch_size, time_steps, features)\n",
    "    score = layers.Dense(1)(score)  # (batch_size, time_steps, 1)\n",
    "    attention_weights = layers.Softmax(axis=1)(score)  # (batch_size, time_steps, 1)\n",
    "    context_vector = layers.Multiply()([inputs, attention_weights])\n",
    "    context_vector = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(context_vector)\n",
    "    return context_vector\n",
    "\n",
    "def build_hybrid_model(metadata):\n",
    "    input_shape = (metadata['max_frames'], metadata['n_features'], 1)\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = residual_block(x, filters=64, kernel_size=(3,3), dilation_rate=2, dropout_rate=0.4)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.TimeDistributed(layers.Flatten())(x)\n",
    "    \n",
    "    # Use Bidirectional LSTM (biLSTM)\n",
    "    x = layers.Bidirectional(\n",
    "            layers.LSTM(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.001))\n",
    "        )(x)\n",
    "    x = layers.Bidirectional(\n",
    "            layers.LSTM(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.001))\n",
    "        )(x)\n",
    "    x = layers.Bidirectional(\n",
    "            layers.LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(0.001))\n",
    "        )(x)\n",
    "    \n",
    "    context_vector = attention_block(x)\n",
    "    \n",
    "    # Classifier\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(context_vector)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(metadata['classes']), activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_callbacks():\n",
    "    return [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, metadata):\n",
    "    model = build_hybrid_model(metadata)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=get_callbacks()\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, metadata):\n",
    "  \n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    \n",
    "    # Show Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d',\n",
    "                xticklabels=metadata['classes'],\n",
    "                yticklabels=metadata['classes'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show Classification Report\n",
    "    print(classification_report(y_true, y_pred, target_names=metadata['classes']))\n",
    "\n",
    "def plot_learning_rate(history):\n",
    "    lr_history = history.history.get('lr')\n",
    "    if lr_history:\n",
    "        plt.plot(lr_history)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.show()\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "        \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='green', linestyle='-')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', color='orange', linestyle='--')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue', linestyle='-')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='red', linestyle='--')\n",
    "    plt.title('Accuracy Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model, history = train_model(X_train, y_train, X_test, y_test, metadata)\n",
    "evaluate_model(model, X_test, y_test, metadata)\n",
    "plot_learning_rate(history)\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
